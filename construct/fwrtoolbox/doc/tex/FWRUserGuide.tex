 %===================================
%
%        oO   FWR Toolbox User's Guide   Oo
%
%===================================
%
% File : FWRUserGuide.tex
% Date : 27dec07
% Author : HILAIRE Thibault
%
% $Id: FWRUserGuide.tex 202 2009-01-03 22:32:11Z hilaire $
%===================================

\documentclass{article}






%========
% packages
%========
\usepackage{THmacros}					% personal macros
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage[latin1]{inputenc}			% french accents
\usepackage{graphicx}					% pdf logo
	\graphicspath{{fig/}}
\usepackage{theorem}					% for definition, remarks
\usepackage{mathrsfs}					% for \mathscr font
\usepackage{hhline}						% hhline macro (in tabular)
\usepackage{pmat}						% for dashed matrix
\usepackage	[						%
	hyperindex = true, 					%
	breaklinks = true, 					%
	colorlinks = true, 					%
	urlcolor = blue, 						%
	linkcolor = blue, 					%
	citecolor = green,					%
	bookmarks = true, 					%
	bookmarksopen = true, 				%
	bookmarksopenlevel = 1,				%
	naturalnames = true					%
			]{hyperref}					% links in pdf
\usepackage{algorithm2e}
\usepackage{dsfont}						% \mathds{1} replace \mathbb{1}
\usepackage{multicol}
\usepackage{array}
\usepackage{longtable}
\usepackage{listings}

\lstset{breaklines=true,tabsize=4,basicstyle=\ttfamily,columns=flexible}		%listing definition

%=======
% macros
%=======
\newcommand{\matlab}[1]{\texttt{#1}}		% display a matlab name
\theorembodyfont{\itshape}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{problem}{Problem}

%list of functions
%== definition of a command ==
\newenvironment{command}[2][\DefaultOptCommand]{%
	\def\DefaultOptCommand{#2}%
	\subsubsection{\matlab{#2}}\hypertarget{command:#1}{}%
}
{}%{\newpage}

\newcommand{\desc}[1]{\paragraph*{#1 :}\ \\}

\newcommand{\funcName}[2][\DefaultOptCommand]{%
	\def\DefaultOptCommand{#2}%
	\matlab{\hyperlink{command:#1}{#2}}%
}

%hyphenation
\hyphenation{pa-ra-me-tri-za-tion}



%===========
\begin{document}


%======
% cover
%======
\thispagestyle{empty}
\begin{center}
	\includegraphics[scale=0.5]{construct/FWRlogo.pdf}\\
	\Huge Finite Wordlength Realizations Toolbox User's Guide\\
	v 0.99\\
%	{\footnotesize\I{(still beta version)}\\}
%	\vspace{1cm}
%	\Large
%	\textsc{Thibault HILAIRE}\\
%	\href{mailto:thibault.hilaire@irisa.fr}{\texttt{thibault.hilaire@irisa.fr}} \\
%	\large
%	\textsc{Cairn project, INRIA/IRISA\\ 6 rue K\'erampont, 22305 LANNION - Frane}\\
%	\Large
%	\vspace{0.5cm}
	\vspace{3cm}
	\Large\url{http://fwrtoolbox.gforge.inria.fr/}
\end{center}


%==============
% Table of Contents
%==============
\newpage\ \newpage
\tableofcontents
\newpage


%========
% Abstract
%========
\section*{Abstract}
The FWR Toolbox is a MATLAB toolbox used to analyze  the Finite Word Length effects of linear time-invariant digital   filters/controllers implementations.\\
When digital filter/controller are implemented in computing machines (micro-controller, DSP, FPGA, etc.) with finite precision, a degradation occurs. It comes from:
\begin{itemize}
	\item  the addition of roundoff noise after each arithmetic operation,
	\item  the rounding of the embedded parameters.
\end{itemize}
The FWR Toolbox provides a general description of any possible realization (direct form, state-space, $\delta$ or shift operator, observer-state-feedback, cascad decomposition, etc...) in a form that allows a straightforward analysis of the FWL effects.
Several tools are provided to compute open-loop/closed-loop sensitivity, related stability measure, roundoff noise gain, ..., and to find \I{optimal} realizations, according to these criteria. It is also possible to generate fixed-point code (C code, VHDL, ...).


%This toolbox is based on the work done during the PhD thesis of Thibault \textsc{Hilaire}, (Philippe \textsc{Chevrel} and Yvon \textsc{Trinquet} as advisors) with the IRCCyN Lab (\I{Institut de Recherche en Communication et en Cybern\'etique de Nantes}, France) and PSA Peugeot-Citro\"en and the work done at IRISA Lab (\I{nstitut de Recherche en Informatique et Systèmes Aléatoires}, France) in the Cairn (ex-R2D2) project.

\newpage


%=======
% Licence
%=======
\section*{License}
Copyright \textsc{T. Hilaire} (\href{mailto:thibault.hilaire@nt.tuwien.ac.at}{\texttt{thibault.hilaire@nt.tuwien.ac.at}}), Institute of Communications and Radio-Frequency Engineering, Vienna University of Technology, Austria.\\


This software is governed by the CeCILL-C license under French law and abides by the rules of distribution of free software. You can use, modify and/ or redistribute the software under the terms of the CeCILL-C license as circulated by CEA, CNRS and INRIA at the following URL \url{http://www.cecill.info}.

As a counterpart to the access to the source code and  rights to copy, modify and redistribute granted by the license, users are provided only with a limited warranty  and the software's author, the holder of the economic rights, and the successive licensors have only limited liability.\\
In this respect, the user's attention is drawn to the risks associated with loading, using, modifying and/or developing or reproducing the software by the user in light of its specific status of free software, that may mean that it is complicated to manipulate, and that also therefore means that it is reserved for developers and experienced professionals having in-depth computer knowledge. Users are therefore encouraged to load and test the software's suitability as regards their
requirements in conditions enabling the security of their systems and/or data to be ensured and,  more generally, to use and operate it in the same conditions as regards security.

The fact that you are presently reading this means that you have had knowledge of the CeCILL-C license and that you accept its terms. A copy of the CeCILL-C license is given in the \texttt{COPYING} file of the distribution.\\

\noindent This product could include software (\texttt{ASA}) developed by Lester Ingber and other contributors : \url{http://www.ingber.com/}.\\
This product could include software (\texttt{Asamin}) developed by Shinichi Sakata : \url{http://www.econ.lsa.umich.edu/~sakata/software}.


\section*{Authors}

\begin{itemize}
	\item[$\bullet$] Thibault HILAIRE (main contributor): \\\href{mailto:thibault.hilaire@nt.tuwien.ac.at}{\texttt{thibault.hilaire@nt.tuwien.ac.at}}, Institute of Communications and Radio-Frequency Engineering, Vienna University of Technology, Austria.
	\item[$\bullet$] Yu FENG:\\ \href{mailto:yu.feng@emn.fr}{\texttt{yu.feng@emn.fr}}, \'Ecole des Mines de Nantes, France.
\end{itemize}



\section*{Acknowledgment}

This toolbox is based on
\begin{itemize}
	\item the work done during Thibault \textsc{Hilaire}'s PhD thesis, (Philippe \textsc{Chevrel} and Yvon \textsc{Trinquet} as advisors) with the IRCCyN Lab (\I{Institut de Recherche en Communication et en Cybern\'etique de Nantes}, France) and PSA Peugeot-Citro\"en
	\item the work done at IRISA Lab (\I{nstitut de Recherche en Informatique et Systèmes Aléatoires}, France) in the Cairn (ex-R2D2) project.
	\item the work done by Yu Feng during his Master's thesis at IRCCyN Lab.
\end{itemize}
This toolbox is now maintained (and still enriched) during my postdoc position at the Institute of Communications and Radio-Frequency Engineering, Vienna University of Technology, Austria.

The logo comes from \url{http://environnement.ecoles.free.fr}.

The authors would like to thank people who help us for this toolbox, directly or indirectly:
\begin{itemize}
	\item Philippe \textsc{Chevrel} -- IRCCyN Lab  and École des Mines de Nantes, France
	\item Olivier \textsc{Sentieys}, Daniel \textsc{Ménard} -- INRIA-IRISA Cairn Team and Université de Rennes 1, France
	\item James \textsc{Whidborne} -- Department of Aerospace Science, Cranfield University, UK
\end{itemize}

\newpage


%=========
% Installation
%=========
\section{Installation}
Instructions for installing the FWR Toolbox can be found in the section entitled \I{Installing Toolboxes} in the computer specific section of the MATLAB user's guide.\\
We recommend that you locate the files from this toolbox in a directory named \matlab{FWRToolbox} of the toolbox directory and add it in the MATLAB path.

This toolbox may require the \I{Adaptative Simulating Annealing} (\texttt{ASA}) software (\url{http://www.ingber.com/}) and its Matlab's gateway (\texttt{Asamin} \url{http://www.econ.ubc.ca/ssakata/public_html/software}) in order to find optimal realization with the global optimization algorithm ASA.\\

The latest version of the toolbox can be download with the anonymous Subversion access with the following command (or with your favorite SVN client):
\begin{center}
	\texttt{svn checkout svn://scm.gforge.inria.fr/svn/fwrtoolbox}
\end{center}

Warning : the \texttt{Control System Toolbox} is required in order to use the FWR Toolbox.





%==========
% Introduction
%==========
\section{Introduction to FWL problem}

% - FWL problem
% - measures
% - optimal design pb


% the introduction comes from the TCAS-I paper
When digital filters are implemented, they are implemented with finite precision due to the finite wordlength (FWL) of the representation of numbers within the computing machine. There are two FWL effects. The first is the addition of noise into the system resulting from the rounding of variables before and after each arithmetic operation - the ``round-off noise''. The second is the degradation in the performance and/or the stability resulting from rounding of the filter coefficients - the ``coefficient sensitivity''. The FWL problem is hence to analyze the effects to ensure that they do not cause significant deterioration in the performance of an implemented filter. The effects are obviously dependent upon the chosen wordlength and on the chosen arithmetic format (floating-point, fixed-point, etc.). Slightly less obvious is the fact that the FWL effects are very dependent upon the particular realization, (direct form, cascade, etc.), and upon the chosen operator (shift operator, $\delta$ operator, etc.). Thus in seeking to alleviate the FWL effects, the realization must also be considered.

The FWL effects have been studied for many years. Although many of the early works were motivated by problems in control systems \cite{Bert58,SlaughterTAC64},  the analysis of the effects were often considered in the open loop. See \cite{LiuTCT71} for a comprehensive review of early work. Further reviews can be found in \cite{Will92,Geve93,IstepanianDCIF}. There has also been a large amount of work that considers the problem of round-off noise (e.g.\ \cite{SandbergBSTJ67,Mull76,Hwan77}).

Early consideration of the transfer function sensitivity to rounding errors in the coefficients can be found in \cite{KnowlesOlcaytoTCAS68,AgarwalBurrusTCAS75}. The work of Thiele \cite{Tavs84,Thiele84,Thiele86} is particularly important in defining a norm on the input-output sensitivity that is tractable. This sensitivity measure provides the foundation for much of the subsequent work. Solutions for other similar  measures can be found in \cite{Geve93,YanMooreCAS92} and further developed in, for example, \cite{LiAndersonGeversPerkins92,XiaoTSP97,Hina02a}. A related measure using a statistical analysis of the input-output sensitivity has been developed \cite{IwatsukiTCAS90}. An extension to the multivariable system case is provided in \cite{LutzHakimiTCAS88}. The closed-loop control case has  also been considered, for example in \cite{MadievskiAndersonGeversAUTO95}. Methods for the simultaneous minimization of a sensitivity measure with round-off noise \cite{LuHinamotoTSP2005} and subject to scaling requirements \cite{HinamotoTCASII2005} have also been developed recently.

The sensitivity of the  poles (and zeros) is also a commonly used measure of the coefficient rounding effect. An early analysis appears in \cite{Kaiser66}. Mantey \cite{ManteyTAC68} showed that the poles/eigenvalues are dependent on the state-space realization. It is well-known that an eigenvalue sensitivity  is minimized if the system is normal \cite{Skel84}. However  Gevers and Li \cite{Geve93} subsequently determined the realization that would minimize a pole sensitivity measure combined with a zero sensitivity measure proposed in \cite{Will86}. Much subsequent work (see \cite{Li98,Whid01b,Wu01,Ko04a}, for example) has considered various similar eigenvalue sensitivity measures for closed-loop control systems.

Most of the significant results have expressed the filter in the state space form. Although \B{most realizations can be transformed into the state-space form,  this form is not completely general and has several limitations}. Firstly, the analysis of the rounding effect of a specific coefficient in a particular realization form can become very difficult after transformation to the state space form. Secondly, many realization forms require the computation of intermediate variables that cannot be expressed in the state-space form. Furthermore, the state space form is specific to the chosen operator. In reality all implementable operators are actually implemented using the shift operator. For example, a realization expressed in the form of a $\delta$-operator is actually implemented using a shift operator in combination with an intermediate variable.

Thus a description that includes intermediate variables is required. The FWR Toolbox proposes a particular implicit state-space description that is not subject to these limitations. The proposed specialized implicit form provides a generalized description of any realization in a form that allows a straightforward analysis of the FWL effects as will be shown in section \ref{chapt:implicit}.   The description is macroscopic in that it does not require coding details and is platform independent but gives a  direct relationship between the description and the implementation algorithm.
Note that the idea of representing the intermediate variables in the description has been considered previously \cite{ChanTAC80} (see also \cite{ChanICASSP79,MoroneyWillskyHouptTAC80}), but the description form is less general than the implicit form considered in this paper. For example, $\delta$-realizations cannot be described using this form.

%Some FWL measures, like the transfer function sensitivity, the pole sensitivity or the roundoff noise gain can be expressed for




%==============
% The Implicit form
%==============
\section{A unifying framework}\label{chapt:implicit}

%======================================
\subsection{The Specialized Implicit Framework (SIF)}

To show the utility of the implicit realization, we consider an example of the implementation of a $\delta$-operator state-space realization. It is well-known \cite{Geve93,Midd90a,GoodallRIC2001} that the $\delta$-operator is  numerically superior to the usual shift operator  generally resulting in less sensitive implementations with less rounding noise.

For a realization expressed with the $\delta$-operator, the input/output relation is
\begin{equation}\label{eq:delta_realization}
	\left\lbrace\begin{array}{rcl}
		\delta[X(k)] &=& A_\delta X(k) + B_\delta U(k) \\
		Y(k) &=& C_\delta X(k) + D_\delta U(k)
	\end{array}\right.
\end{equation}
with $\delta=\frac{q-1}{\Delta}$, where $\Delta$ is a strictly positive constant
and $q$ is the delay operator \cite{Geve93}.
This is equivalent, in infinite precision, to the classical state-space realization
\begin{equation}\label{eq:q_realization}
	\left\lbrace\begin{array}{rcl}
		q[X(k)] &=& A_q X(k) + B_q U(k) \\
		Y(k) &=& C_q X(k) + D_q U(k)
	\end{array}\right.
\end{equation}
with $A_q=\Delta A_\delta+I$, $B_q=\Delta B_\delta$, $C_q=C_\delta$ and $D_q=D_\delta$.


With these two equivalent realizations, the parametrization is different, therefore when the parameters are subjected to FWL rounding, the two realizations are no longer equivalent, and the impact of the quantization is different. In addition, in order to implement  the $\delta$-operator,  intermediate variables are necessary.  These  are also subject to FWL quantization. So the following algorithm
\begin{align}\label{eq:delta_algorithm}
	T &\leftarrow A_\delta X(k) + B_\delta U(k) \nonumber\\
	X(k+1) &\leftarrow X(k) + \Delta T \\
	Y(k) &\leftarrow C_\delta X(k) + D_\delta U(k) \nonumber
\end{align}
implements \eqref{eq:delta_realization} where $T$ is an intermediate variable vector.

There are many other possible implementation forms, such  as  direct form I or II, cascade/parallel decomposition, lattice filters, mixed $q$/$\delta$, etc., and many of these also require intermediate variables. In order to consider all of them within a general unifying framework, we propose a description, in a single equation, of the filter implementation. The equation provides an explicit description of the parametrization, and allows the analysis of the FWL effects, but is still a macroscopic description. Furthermore, the description is given within a formalism such that the description takes the form of an implicit state-space system. This specialized implicit framework (SIF) is given by	
\begin{equation}\label{eq:def_implicit}
	\begin{pmatrix}
		J\hspace{-1mm} & 0 & 0\\
		-K\hspace{-1mm} & I_{n}\hspace{-2mm} & 0\\
		-L\hspace{-1mm} & 0 & I_{p}
	\end{pmatrix}\hspace{-1mm}
	\begin{pmatrix}
		T(k+1)\\
		X(k+1)\\
		Y(k)
	\end{pmatrix}\hspace{-1mm}
	=\hspace{-1mm}
	\begin{pmatrix}
		0 & M & N\\
		0 & P & Q\\
		0 & R & S\\
	\end{pmatrix}\hspace{-1mm}
	\begin{pmatrix}
		T(k)\\
		X(k)\\
		U(k)
	\end{pmatrix}
\end{equation}
where
\begin{itemize}
 	\item $J \in \Rbb{l}{l}$, $K \in \Rbb{n}{l}$, $L \in \Rbb{p}{l}$, $M \in \Rbb{l}{n}$, $N \in \Rbb{l}{m}$, $P \in \Rbb{n}{n}$, $Q \in \Rbb{n}{m}$, $R \in \Rbb{p}{n}$, $S \in \Rbb{p}{m}$, $T(k) \in \mathbb{R}^{l}$, $X(k) \in \mathbb{R}^{n}$, $U(k) \in \mathbb{R}^{m}$ and $Y(k) \in \mathbb{R}^{p}$,
	\item  matrix $J$ is lower triangular with $1$'s on the diagonal, i.e.
		 \begin{equation}\label{eq:defJ}
		 	J = \begin{pmatrix}
		 		1 		& 0 		& \hdots 	& 0\\
		 		\star	& 1    	&		    	&  0\\
		 		\vdots	& 		&  \ddots	& \vdots\\
		 		\star	& \star & \hdots	& 1
		 	\end{pmatrix},
		 \end{equation}%
	\item $T(k+1)$ is the intermediate variable in the calculations of step $k$ (the column of $0$'s in the second matrix shows that $T(k)$ is not used for the calculation at step $k$ -- this characterizes the concept of an intermediate variable),
	\item $X(k+1)$ is the stored state-vector ($X(k)$ is effectively stored from one step to the next, in order to compute $X(k+1)$ at step $k$).
\end{itemize}
$T(k+1)$ and $X(k+1)$ form the descriptor-vector: $X(k+1)$ is stored from one step to the next, while $T(k+1)$ is computed and used within one time step.


\par It is implicitly assumed  that the computations associated with the realization \eqref{eq:def_implicit} are executed in row order giving the following algorithm:
\begin{align*}
	&\text{[i]} & JT(k+1) & \leftarrow MX(k) + NU(k)\\
	&\text{[ii]} & X(k+1)  & \leftarrow KT(k+1) + PX(k) + QU(k)\\
	&\text{[iii]} & Y(k)    & \leftarrow LT(k+1) + RX(k) + SU(k)
\end{align*}
Note that in practice, steps [ii] and [iii] could be exchanged to reduce the computational delay.
Also note that because the computations are executed in row order and $J$ is lower triangular with $1$'s on the diagonal, there is no need to compute $J^{-1}$.%  The example in \ref{chapt:example} shows how to exploit this particularity that gives an extra degree of freedom (eq. \eqref{eq:example_Emplicit}).

Equation \eqref{eq:def_implicit} is equivalent in infinite precision to the classical state-space form
\begin{equation}\label{eq:eq_form}
	\begin{pmatrix}
		T(k+1)\\X(k+1)\\ \hline Y(k)
	\end{pmatrix}
	=
	\left(\begin{array}{cc|c}
		0 & J^{-1}M & J^{-1}N \\
		0 & A_Z & B_Z \\ \hline
		0 & C_Z & D_Z
	\end{array}\right)
	\begin{pmatrix}
		T(k) \\ X(k) \\ \hline U(k)
	\end{pmatrix}
\end{equation}
with $A_Z\in\Rbb{n}{n}$, $B_Z\in\Rbb{n}{m}$, $C_Z\in\Rbb{p}{n}$ and $D_Z\in\Rbb{p}{m}$ where
\begin{align}
	A_Z &= KJ^{-1}M+P,   &  B_Z &= KJ^{-1}N+Q,   \label{eq:defAZandBZ}\\	
	C_Z &= LJ^{-1}M+R, &  	D_Z &= LJ^{-1}N+S. \label{eq:defCZandDZ}
\end{align}

%	\begin{equation}
%		\begin{pmatrix}
%			A & B \\
%			C & D
%		\end{pmatrix}
%		=
%		\begin{pmatrix}
%			K\\
%			L
%		\end{pmatrix}
%		J^{-1}
%		\begin{pmatrix}
%			M & N
%		\end{pmatrix}
%		+
%		\begin{pmatrix}
%			P & Q \\
%			R & S
%		\end{pmatrix}
%	\end{equation}
Note that \eqref{eq:eq_form} corresponds to a different parame\-trization than  \eqref{eq:def_implicit}.
The system transfer function is given by
\begin{equation}\label{eq:tf}
	H(z)=C_Z(zI_n-A_Z)^{-1}B_Z+D_Z.
\end{equation}


%===============
\subsection{definitions}

	To complete the framework, the following definitions are required.

\begin{definition}\label{def:realization}
	A \B{realization}, $\mathcal{R}$, is defined by the specific set of matrices $J$, $K$, $L$, $M$, $N$, $P$, $Q$, $R$ and $S$ used to describe a realization with the implicit form of \eqref{eq:def_implicit} :
	\begin{equation}
		\mathcal{R} :\triangleq (J,K,L,M,N,P,Q,R,S).
	\end{equation}
\end{definition}
\begin{remark}
$\mathcal{R}$ can also be defined by the matrix $Z\in\Rbb{(l+n+p)}{(l+n+m)}$
\begin{equation}\label{eq:def_Z}
	Z \triangleq \begin{pmatrix} -J & M & N \\ K & P & Q \\ L & R & S \end{pmatrix}
\end{equation}
and the dimensions $l$, $m$, $n$ and $p$, so $\mathcal{R}$ could be defined by $\mathcal{R}:=(Z,l,m,n,p)$.
\end{remark}

\begin{definition}
	$\mathscr{R}_{H}$ denotes the set of realizations with transfer function $H$. These realizations are said to be equivalent.
\end{definition}


In order to encompass realizations with some special structure ($q$-operator  state-space, $\delta$-operator  state-space, direct form, cascade, lattice filters, etc.), we define a set of realizations that possess a particular structure.

\begin{definition}\label{def:structuration}
	A \B{structuration}\footnote{This is a useful French word that we have purloined. It is also used in the field of social sciences. Here it means the set of structured realizations.} $\mathscr{S}$ is a set of realizations having a common structure: some coefficients or some dimensions are fixed \emph{a priori}.
\end{definition}
Some examples of common structurations are given in the next section.

\begin{definition}
	$\mathscr{R}^{\mathscr{S}}_{H}$ is the set of equivalent structured realizations. Realizations from $\mathscr{R}^{\mathscr{S}}_{H}$ are structured according to $\mathscr{S}$ and have a transfer function $H$. Hence $	 \mathscr{R}^{\mathscr{S}}_{H} \triangleq \mathscr{R}_{H} \cap \mathscr{S}$.
% 	\begin{equation}
% 		\mathscr{R}^{\mathscr{S}}_{H} \triangleq \mathscr{R}_{H} \cap \mathscr{S}.
% 	\end{equation}
\end{definition}


\begin{definition}
	A \B{parametrization} of a realization $\mathcal{R}$ is the set of  coefficients of $Z$ that are significant for the realization.
\end{definition}

%For example, with the $q$-operator state-space realization of \eqref{eq:q_realization}, the parametrization is given by the matrices $A_q$, $B_q$, $C_q$ and $D_q$. But for the $\delta$-operator state-space realization of \eqref{eq:delta_realization}, the parametrization is given by the matrices $A_\delta$, $B_\delta$, $C_\delta$, $D_\delta$ and the parameter $\Delta$. %We will see in the next section that the $\delta$-operator state-space realization includes some additional parameters that are always set to unity or zero. These are not `significant coefficients' and hence are not included in the parametrization.

%===============
\subsection{Examples}

	\subsubsection{Classical state-space}
	
The classical state-space realization can, of course, be expressed with the SIF.\\
The realization
\begin{equation}\label{eq:ss2}
	\left\lbrace\begin{array}{rcl}
		X(k+1) &=& A_qX(k) + B_qU(k) \\
		Y(k) &=& C_qX(k) + D_qU(k)
	\end{array}\right.
 \end{equation}
correspond to the implicit form with $l=0$, So
%\begin{equation}
%	\begin{pmatrix}
%		. & . & .\\
%		. & I_{n} & 0\\
%		. & 0 & I_m
%	\end{pmatrix}
%	\begin{pmatrix}
%		T(k+1)\\
%		X(k+1)\\
%		Y(k)
%	\end{pmatrix}
%	=
%	\begin{pmatrix}
%		. & . & .\\
%		. & A_q & B_q\\
%		. & C_q & D_q\\
%	\end{pmatrix}
%	\begin{pmatrix}
%		T(k)\\
%		X(k)\\
%		U(k)
%	\end{pmatrix}
%\end{equation}
%ou encore
\begin{equation}
	Z =
	\begin{pmatrix}
		. & . & . \\
		. & A_q & B_q \\
		. & C_q & D_q
	\end{pmatrix}
\end{equation}

	
	
	
	\subsubsection{State-space with $\delta$ operator}


The $\delta$-state-space realization corresponds to
\begin{equation}
	\left\lbrace\begin{array}{rcl}
		\delta[X(k)] &=& A_\delta X(k) + B_\delta U(k) \\
		Y(k) &=& C_\delta X(k) + D_\delta U(k)
	\end{array}\right.
\end{equation}
with $\delta = \frac{q-1}{\Delta}$ and $\Delta>0$.\\
This is realized with
\begin{equation}\label{eq:implicit_delta2}
	\begin{pmatrix}
		I_{n} & 0 & 0\\
		-\Delta I_{n} & I_{n} & 0\\
		0 & 0 & I_{p}
	\end{pmatrix}
	\begin{pmatrix}
		T(k+1)\\
		X(k+1)\\
		Y(k)
	\end{pmatrix}
	=
	\begin{pmatrix}
		0 & A_\delta & B_\delta\\
		0 & I_{n} & 0\\
		0 & C_\delta & D_\delta\\
	\end{pmatrix}
	\begin{pmatrix}
		T(k)\\
		X(k)\\
		U(k)
	\end{pmatrix}
\end{equation}



	\subsubsection{Cascade decomposition}
	Let's consider two systems $\mathcal{R}_1$ and $\mathcal{R}_2$ with the following SIF expression:
\begin{equation}
	\begin{pmatrix}
		J_1  & 0 & 0 \\
		-K_1 & I  & 0 \\
		-L_1 & 0 & I
	\end{pmatrix}
	\begin{pmatrix}
		T_1(k+1) \\
		X_1(k+1) \\
		Y_1(k)
	\end{pmatrix}
	=
	\begin{pmatrix}
		0 & M_1 & N_1 \\
		0 & P_1 & Q_1 \\
		0 & R_1 & S_1 \\
	\end{pmatrix}
	\begin{pmatrix}
		T_1(k) \\
		X_1(k) \\
		U_1(k)
	\end{pmatrix}
\end{equation}
\begin{equation}
	\begin{pmatrix}
		J_2  & 0 & 0 \\
		-K_2 & I  & 0 \\
		-L_2 & 0 & I
	\end{pmatrix}
	\begin{pmatrix}
		T_2(k+1) \\
		X_2(k+1) \\
		Y_2(k)
	\end{pmatrix}
	=
	\begin{pmatrix}
		0 & M_2 & N_2 \\
		0 & P_2 & Q_2 \\
		0 & R_2 & S_2 \\
	\end{pmatrix}
	\begin{pmatrix}
		T_2(k) \\
		X_2(k) \\
		U_2(k)
	\end{pmatrix}
\end{equation}	
\fig[scale=0.35]{cascade}{Two systems cascaded}
The two systems are put on cascade.\\
The intermediate variables receive the result of the first system, so the cascaded system has the following SIF realization:
\begin{footnotesize}	
	\begin{equation*}
		\begin{pmat}({..|.|})
			J_1 & 0 & 0 & 0 & 0 & 0 \cr
			-L_1 & I & 0 & 0 &0 & 0 \cr
			0 & -N_2 & J_2 & 0 & 0 & 0 \cr\-
			-K_1 & 0 & 0 & I & 0 & 0 \cr
			0 & -Q_2 & -K_2 & 0 & I & 0 \cr\-
			0 & -S_2 & -L_2 & 0 & 0 & I \cr
		\end{pmat}
		\begin{pmatrix}
			T_1(k+1) \\
			T(k+1) \\
			T_2(k+1) \\
			X_1(k+1) \\
			X_2(k+1) \\
			Y_2(k)
		\end{pmatrix}
		=
		\begin{pmat}({..|.|})
			0 & 0 & 0 & M_1 & 0 & N_1 \cr
			0 & 0 & 0 & R_1 & 0 & S_1 \cr
			0 & 0 & 0 & 0 & M_2 & 0 \cr\-
			0 & 0 & 0 & P_1 & 0 & Q_1 \cr
			0 & 0 & 0 & 0 & P_2 & 0 \cr\-
			0 & 0 & 0 & 0 & R_2 & 0 \cr
		\end{pmat}
		\begin{pmatrix}
			T_1(k) \\
			T(k) \\
			T_2(k) \\
			X_1(k) \\
			X_2(k) \\
			U_1(k)
		\end{pmatrix}
	\end{equation*}
\end{footnotesize}
So
\begin{equation}
	Z = \begin{pmat}({..|.|})
		-J_1 & 0 & 0 & M_1 & 0 & N_1 \cr
		L_1 & -I & 0 & R_1 & 0 & S_1 \cr
		0 & N_2 & -J_2  & 0 & M_2 & 0 \cr\-
		K_1 & 0 & 0 & P_1 & 0 & Q_1 \cr
		0 & Q_2 & K_2 & 0 & P_2 & 0 \cr\-
		0 & S_2 & L_2 & 0 & R_2 & 0 \cr
	\end{pmat}
\end{equation}

	

\subsubsection{Others forms}

A lot of other possible structurations are considered. Here are some of them:
\begin{itemize}
	\item Direct Form I with $q$-operator (\funcName{DFIq2FWR})
	\item $\rho$ Direct Form II transposed (\funcName{rhoDFIIt2FWR}), that encompasses the Direct Form II with $q$ and $\delta$-operators
	\item cascade and parallel decomposition (\funcName[@FWR/plus]{plus}, \funcName[@FWR/mtimes]{mtimes})
	\item classical state-space realizations (\funcName{SS2FWR})
	\item $\delta$-state-space realizations (\funcName{SSdelta2FWR})
	\item Modal form with $\rho$-operator (\funcName{Modalrho2FWR})
	\item ...
\end{itemize}


%======================
\subsection{Equivalent classes}
\newcommand{\mt}[1]{\mathcal{#1}}

In order to exploit the potential offered by the specialized implicit form in improving implementations, it is necessary to describe sets of equivalent system realizations. However, non-minimal realizations  may provide better implementations (the $\delta$-form can be seen as a non-minimal realization when expressed in the implicit state-space form with the shift operator. Hence the notion of equivalence needs to be extended so that the system state dimension does not need to be preserved.
The \I{Inclusion Principle}, introduced by \v{S}iljak and Ikeda \cite{Iked84,Silj91} in the context of  decentralized control, has been used to allows the formalization of the \I{equivalence} and \I{inclusion} relations between two realizations $\mathcal{R}$ and $\tilde{\mathcal{R}}$ (see \cite{Hila07b}).

Although this extension of the \I{Inclusion Principle} gives the formal description of equivalent classes, it is of practical interest to consider realizations of the same dimensions ($\tilde{l}=l$ and $\tilde{n}=n$) where transformations from one realization to another is only a similarity transformation.
\begin{proposition}\label{prop:inclusion_principle2}
	Consider a realization  $\mathcal{R}:=(Z,l,m,n,p)$. % with $l,m,n,p$ as dimension.\\
	All the realizations $\tilde{\mathcal{R}}:=(\tilde{Z},l,m,n,p)$ with
	\begin{equation}\label{eq:UYWtransformation}
		\tilde{Z} =
		\begin{pmatrix}
			\mt{Y}\\
			&\mt{U}^{-1}\\
			&&I_{p}
		\end{pmatrix}
		Z
		\begin{pmatrix}
			\mt{W}\\
			&\mt{U}\\
			&&I_{m}
		\end{pmatrix}
	\end{equation}
	and $\mt{U}$, $\mt{W}$, $\mt{Y}$ are non-singular matrices, are equivalent to $\mathcal{R}$.\\
	Eq. \eqref{eq:UYWtransformation} defines a "$\mt{U}\mt{Y}\mt{W}$-transformation.
\end{proposition}
It is also possible to just consider a subset of similarity transformations that preserve a particular structure, say cascade or delta. These are always a particular case of proposition \ref{prop:inclusion_principle2}.\\
For example, if an initial $\delta$-structured realization $\mathcal{R}:=(Z_0,n,m,n,p)$ is given, the subset of equivalent $\delta$-structured realization is defined by
\begin{equation}\label{eq:RHSdelta}
	\mathscr{R}_{H}^{\mathscr{S}_{\delta}} = \left\{
	\begin{array}{l}
		\mathcal{R}:=(Z,n,m,n,p) \backslash\\
	 	\hspace{3mm}Z=\begin{pmatrix}\mt{U}^{-1}\\ &\mt{U}^{-1}\\ &&I_p\end{pmatrix} Z_0 \begin{pmatrix}\mt{U}\\ &\mt{U}\\ &&I_m\end{pmatrix}\\
		\hspace{3mm}\forall \mt{U}\in\Rbb{n}{n} \text{\ non-singular}
	\end{array}
	 \right\}
\end{equation}
In addition to a description of the various existing realizations with the exact parametrization, this formalism gives an algebraic characterization of equivalent classes. These classes can be used to search for an optimal structured realization (see Section \ref{chapt:measures}).


%=============================================
\subsection{Finite Wordlength measures}\label{chapt:measures}

%
\subsubsection{Coefficient's quantization}
A coefficient's quantization depends both on its value and its representation.

Firstly if the value of a coefficient is such that it will be quantized without error, then that parameter makes no contribution to the overall coefficient sensitivity. Hence we introduce weighting matrices $W_J$ to $W_S$ (and also $W_Z$) respectively associated with matrices $J$ to $S$ of a realization, such that
\begin{equation}\label{eq:defW}
	\pa{W_X}_{i,j} \triangleq
	\begin{cases}
		0 & \text{if $X_{i,j}$ is exactly implemented,}\\
		1 & \text{otherwise.}
	\end{cases}
\end{equation}
%
%Secondly, different representation schemes may be considered. Here we consider both fixed-point and floating-point representations of coefficients expressed using  $\beta$ bits.

%A fixed-point coefficient $x$ is represented by $(-1)^s.N.2^{-\beta_f}$, where $s\in\{0,1\}$, $N$ is an integer coded with $\beta_g$ bits and $\beta_f$ an integer (not stored in the representation) such that $\beta_g+\beta_f+1=\beta$. The quantized $x^\dagger$ of $x$ is such that% $\abs{x^\dagger-x}<2^{-(\beta_f+1)}$.
% \begin{equation}
% 	\abs{x^\dagger-x}<2^{-(\beta_f+1)}.
% \end{equation}

%A floating-point coefficient is represented by $(-1)^s.w.2^e$ where $w\in[0,1[$ (or $w\in[0.5,1[$ for a normalized floating-point representation) and $e$ is an integer coded with $\beta_e$ bits\footnote{The difference with fixed-point is that $e$ is coded with $\beta_e$ bits and can changed. With fixed-point, $\beta_f$ is fixed and implicit.} ($\beta_{e}+\beta_w+1=\beta$). The quantized $x^\dagger$ of $x$ is, in this case, such that %$ \abs{x^\dagger-x}<x.2^{-(\beta_w+1)}$.
% \begin{equation}
% 	\abs{x^\dagger-x}<x.2^{-(\beta_w+1)}.
% \end{equation}

%The choice of $\beta_f$ and $e$ can be unique for each coefficient ($e=\lceil \log_2 \abs{x} \rceil$ and $\beta_f=\beta-1-\lceil \log_2 \abs{x} \rceil$, where $\lceil\cdot\rceil$ is the \I{ceiling} operator).  Alternatively, $\beta_f$ and $e$ are defined for a group of coefficients (in order to reduce the required bit-shifts and the subsequent computational cost). This defines the \emph{block-fixed-point} and \emph{block-floating-point} schemes.
%Following \cite{Wu03a}, we introduce the \I{generalized} dynamic range bit $\beta_r$ ($\beta_r=\beta_g$ or $\beta_e$) and the \I{precision} bit length $\beta_p$ ($\beta_p=\beta_f$ or $\beta_w$).

%Usually, the blocks used in block-representation correspond to the matrices $J$ to $S$, but there is no necessity for this, and blocks can be chosen at will.
%To define the blocks of a realization $\mathcal{R}:=(Z,l,m,n,p)$, we introduce the matrix $\eta_Z$ such that
%\begin{equation}
%	\pa{\eta_Z}_{i,j} \triangleq
%		\left\lbrace\begin{array}{l}
%			\text{the largest absolute value of}\\
%			\text{the block in which }  Z_{i,j} \text{ resides.}
%		\end{array}\right.
%\end{equation}

%This allows a completely general definition of the blocks. Thus there could  be just a single unique block, or every block could consist of only one coefficient.
%For example, denoting
%$\mathscr{E}_{a,b}\in\Rbb{a}{b}$ as a matrix of $1$s and
%\begin{equation}
%	\norm{X}_{\max} \triangleq \underset{i,j}{\max} \abs{X_{i,j}},
%\end{equation}
%then using a block-representation corresponding to the matrices $J$ to $S$ gives
%	\begin{equation*}
%		\eta_Z = \begin{pmatrix}
%			\norm{J}_{\max} \mathscr{E}_{l,l} & \norm{M}_{\max} \mathscr{E}_{l,n} & \norm{N}_{\max} \mathscr{E}_{l,m} \\
%			\norm{K}_{\max} \mathscr{E}_{n,l} & \norm{P}_{\max} \mathscr{E}_{n,n} & \norm{Q}_{\max} \mathscr{E}_{n,m} \\
%			\norm{L}_{\max} \mathscr{E}_{p,l} & \norm{R}_{\max} \mathscr{E}_{p,n} & \norm{S}_{\max} \mathscr{E}_{p,m}		
%		\end{pmatrix}.
%	\end{equation*}
%With a single unique block for $Z$ we get $\eta_Z = \norm{Z}_{\max} \mathscr{E}_{l+n+p,l+n+m}$,
%% \begin{equation}
%% 	\eta_Z = \norm{Z}_{\max} \mathscr{E}_{l+n+p,l+n+m},
%% \end{equation}
%and for one block per coefficient we get $	\pa{\eta_Z}_{i,j} = \abs{Z_{i,j}}$.
%% \begin{equation}
%% 	\pa{\eta_Z}_{i,j} = \abs{Z_{i,j}}.
%% \end{equation}

%\begin{proposition}
%During the quantization process, $Z$ is perturbed to $Z+r_Z\times\Delta$ where
%	\begin{equation}
%		r_Z \triangleq
%		\begin{cases}
%			W_Z & \text{for fixed-point representation,} \\
%			2\eta_Z \times W_Z & \text{for floating-point representation,}
%		\end{cases}
%	\end{equation}
%$\Delta$ is a matrix dependant on the $\beta_p$ \I{precision} bit length, and $\times$ denotes the Schur product.
%If ${\beta_p}_{i,j}$ is the precision bit-length of $Z_{i,j}$, then $\abs{\Delta_{i,j}}<2^{-({\beta_p}_{i,j}+1)}$.
%	% \begin{equation}
%% 		\abs{\Delta_{i,j}}<2^{-({\beta_p}_{i,j}+1)}.
%% 	\end{equation}
%\end{proposition}
%\begin{remark}
%	With this formalism for the different representation schemes, note that the choice of the scale parameter ($e$ or $\beta_f$) is defined for each coefficient ($e_{i,j}=\left\lceil \log_2 \abs{{\eta_Z}_{i,j}} \right\rceil$ and ${\beta_f}_{i,j}=\beta-1-\left\lceil \log_2 \abs{{\eta_Z}_{i,j}} \right\rceil$) and that it is also possible to define the minimum bit length $\beta$ to code each coefficient without overflow or underflow \cite{Hila06c}.
%\end{remark}



In the toolbox, the notation $(\beta,\gamma)$ is used for the fixed-point representation of a variable or coefficient (2's complement scheme), according to figure \ref{fig:bits_vf}: $\beta$ is the total wordlength in bits of the representation, whereas $\gamma$ is the fractional part wordlength (it gives the binary-point position of the representation). They are fixed for each variable and coefficient, and implicit, unlike the floating-point representation. $\beta$ and $\gamma$ will be suffixed by the variable/coefficient they refer to. These parameters could be scalars, vectors or matrices, according to the variables they refer to.
\fig[scale=0.5]{bits_vf}{Fixed-point representation}

To represent a value $x$ without overflow, a fixed-point representation $(\beta_x,\gamma_x)$ may satisfy:
\begin{equation}
	\beta_x-\gamma_x-1 \geq \left\lfloor \strut \log_2|x| \right\rfloor + 1
\end{equation}
where the $\left\lfloor a \right\rfloor$ operation rounds $a$ to the nearest integer lower or equal to $a$ (for positive numbers $\left\lfloor a \right\rfloor$ is the integer part).

In order to simplify the expressions that manipulate wordlengths or binary point positions, matrix extensions of $\log_2$, floor operator $\left\lfloor.\right\rfloor$ and power of $2$ are used. For example, if $M\in\Rbb{p}{q}$, then $\log_2(M)\in\Rbb{p}{q}$ such as $\pa{\log_2(M)}_{i,j} \triangleq \log_2(M_{i,j})$.


%
\subsubsection{Input-Output sensitivity}

 The open-loop transfer function sensitivity measure is defined by
 \begin{equation}
 	M_{L_{2}}^W = \norm{\dede{H}{Z} \times r_{Z}}_{F}^2.
 \end{equation}
 where $\dede{H}{Z}\in\Rbb{l+n+p}{l+n+q}$ is the \I{transfer function sensitivity matrix}.
 It is the matrix of the $L_{2}$-norm of the sensitivity of the transfer function $H$ with
 respect to each coefficient  $Z_{i,j}$. It is defined by
 \begin{equation}
 	\pa{\dede{H}{Z}}_{i,j} \triangleq \norm{\dd{H}{Z_{i,j}}}_{2},
 \end{equation}
  	In SISO case, the $M_{L_{2}}^W$ measure is equal to
 \begin{equation}
 	M_{L_{2}}^W = \norm{\dd{H}{Z} \times r_{Z}}_2^2.
 \end{equation}
 and is then an extension to the SIF of the classical state-space sensitivity measure
 \begin{equation}
 	M_{L_2} \triangleq \norm{\dd{H}{A}}_2^2 + \norm{\dd{H}{B}}_2^2 + \norm{\dd{H}{C}}_2^2.
 \end{equation}
 This measure is implemented with the function \funcName[@FWR/MsensH]{MsensH}.\\
 See \cite{Hila06a},\cite{Hila07b} for more details.


%
\subsubsection{Pole sensitivity}


The pole sensitivity measure of $\mathcal{R}$ is defined by
\begin{equation}
	\Psi = \sum_{k=1}^n \norm{ \dd{\abs{\lambda_k}}{Z} \times r_Z }_F^2.
\end{equation}
(it is also possible to only consider the sensitivity of $\lambda_k$ instead of the sensitivity of $\abs{\lambda_k}$.

A \I{pole sensitivity matrix} can also be constructed to evaluate the overall impact of each coefficient. Let  $\dede{\abs{\lambda}}{Z}$ denote the pole sensitivity matrix defined by
\begin{equation}
	\pa{\dede{\abs{\lambda}}{Z}}_{i,j} \triangleq \sqrt{ \sum_{k=1}^n \pa{\dd{\abs{\lambda_k}}{Z_{i,j}}}^2}.
\end{equation}

Then, The pole sensitivity measure is then given by:
\begin{equation}
	\Psi=\norm{\dede{\abs{\lambda}}{Z} \times r_Z}_F^2.
\end{equation}

This measure is implemented with the function \funcName[@FWR/MsensPole]{MsensPole}.\\
See \cite{Hila06b},\cite{Hila07b} for more details.


%
\subsubsection{Output roundoff noise}

When implemented a realization $\mathcal{R}$, the steps (i) to (iii) are modified by the add of noises $\xi_T(k)$, $\xi_X(k)$ and $\xi_Y(k)$:
\begin{equation}\label{eq:implemented_sys}
	\begin{array}{r>{\hspace{-3mm}}c<{\hspace{-3mm}}l}
		J.T(k+1)  &\leftarrow&  M.X(k) + N.U(k) + \xi_T(k) \\
		X(k+1)  &\leftarrow&  K.T(k+1) + P.X(k) + Q.U(k) + \xi_X(k) \\
		Y(k)  &\leftarrow&  L.T(k+1) + R.X(k) + S.U(k) + \xi_Y(k)
	\end{array}
\end{equation}
These noises added depend on:
\begin{itemize}
	\item the way the computations are organized (the order of the sums) and done,
	\item the fixed-point representation of the inputs, the outputs,
	\item and the fixed-point representation chosen for the states, the intermediate variables and the coefficients.
\end{itemize}
The add of these noises are equivalent to a noise $\xi'(k)$ added on the output.
The Output Noise Power is defined as the power of $\xi'(k)$:
\begin{equation}
	P \triangleq E{ \xi'(k)\xi'(k)^\top }
\end{equation}
where the $E{.}$ is the mean operator.\\


The Roundoff Noise Gain is the output noise power computed in a specific computational scheme : the noises are supposed to
appear only after each multiplication and are modeled by centered white noise statistically independent. Each noise is
supposed to have the same power $\sigma_0^2$ (determined by the wordlength chosen for all the variables and coefficients).

The Roundoff Noise Gain is defined by
\begin{equation}
	G \triangleq \frac{P}{\sigma_0^2}
\end{equation}

These measures are implemented with the function \funcName[@FWR/ONP]{ONP} and \funcName[@FWR/RNG]{RNG}. See \cite{Hila07c} and \cite{Hila08c}.


%
\subsubsection{Closed-loop measures}\label{sec:closed_loop}

\newcommand{\np}{\ensuremath{n_{\mathcal{P}}}}	


These measures are also extended to the closed-loop context. They now concern the closed-loop transfer function or the closed-loop poles.

Consider the plant $\mathcal{P}$ together with the controller $\mathcal{C}$ according to the standard form shown in Figure~\ref{fig:standard_form}, where $W(k)\in\mathbb{R}^{p_1}$ is the exogenous input,  $Y(k)\in\mathbb{R}^{p_2}$ the control input, $Z(k)\in\mathbb{R}^{m_1}$ the controlled output and $U(k)\in\mathbb{R}^{m_2}$ the measured output.

\begin{figure}[!htbp]
	\centering
		\includegraphics[width=0.6\textwidth]{standard_form.pdf}
		\caption{Closed-loop control system}
	\label{fig:standard_form}
\end{figure}

The controller is defined as $\mathcal{C}:=(Z,l,m_2,n,p_2)$ and the plant $\mathcal{P}$ as
\begin{equation}\label{eq:standard_form}
	\mathcal{P}:=
	\left(\begin{array}{c|cc}
		A & B_1 & B_2 \\
		\hline
		C_1 & D_{11} & D_{12} \\
		C_2 & D_{21} & 0		
	\end{array}\right)
\end{equation}
where $A\in\Rbb{\np}{\np}$, $B_1\in\Rbb{\np}{p_1}$, $B_2\in\Rbb{\np}{p_2}$, $C_1\in\Rbb{m_1}{\np}$, $C_2\in\Rbb{m_2}{\np}$, $D_{11}\in\Rbb{m_1}{p_1}$, $D_{12}\in\Rbb{m_1}{p_2}$, $D_{21}\in\Rbb{m_2}{p_1}$ and 	 $D_{22}\in\Rbb{m_2}{p_2}$ is assumed to be zero only to simplify the mathematical expressions.


The closed-loop system $\bar{\mathcal{S}}$ is then given by
\begin{equation}
	\bar{\mathcal{S}}=F_l(\mathcal{P},\mathcal{C}):=
	\left(\begin{array}{c|cc}
		\bar{A} & \bar{B} \\
		\hline\\[-3mm]
		\bar{C} & \bar{D}
	\end{array}\right)	
\end{equation}
%\cite[p. 8]{Alaz99a}
where  $F_l(\cdot,\cdot)$ is the well-known lower linear fractional transform \cite{Zhou96} and where $\bar{A}\in\Rbb{\np+n}{\np+n}$, $\bar{B}\in\Rbb{\np+n}{p_1}$, $\bar{C}\in\Rbb{m_1}{\np+n}$ and $\bar{D}\in\Rbb{m_1}{p_1}$ are such that
\begin{align}
		\bar{A}&= \begin{pmatrix}
			A + B_2 D_Z C_2	&\quad	B_2 C_Z \\
			B_Z C_2	&\quad	A_Z
		\end{pmatrix}, \label{eq:defAbar} &
		\bar{B} &= \begin{pmatrix}
			B_1 + B_2 D_Z D_{21} \\
			B_Z  D_{21}
		\end{pmatrix},\\
		\bar{C} &= \begin{pmatrix}
			C_1 + D_{12} D_Z C_2	&\quad	D_{12} C_Z
		\end{pmatrix}, &
		\bar{D} &=  D_{11} + D_{12} D_Z D_{21}.
\end{align}
The closed-loop transfer function is
\begin{equation}
%	\bar{H} : z \to \bar{C}\pa{zI_{\np+n}-\bar{A}}^{-1}\bar{B}+\bar{D}
% ?? slightly neater and dimension of I is obvious ??
	\bar{H} : z \mapsto \bar{C}\pa{zI -\bar{A}}^{-1}\bar{B}+\bar{D}.
\end{equation}


These notations will be used  in the toolbox.\\
Then, the following measures are considered:
\begin{itemize}
	\item the input-output sensitivity (\funcName[@FWR/MsensHcl]{MsensH\_cl})
		\begin{equation}
		 	\bar{M}_{L_{2}}^W = \norm{\dd{\bar{H}}{Z} \times r_{Z}}_2^2.
		 \end{equation}	
	\item the pole-sensitivity and related stability measure (\funcName[@FWR/MsensPolecl]{MsensPole\_cl} and \funcName[@FWR/Mstability]{Mstability})
	 	\begin{equation}
	 		\bar{\Psi} = \sum_{k=1}^n \norm{ \dd{\abs{\bar{\lambda}_k}}{Z} \times r_Z }_F^2.
	 	\end{equation}
	\item the roundoff noise gain (\funcName[@FWR/RNGcl]{RNG\_cl})
	
\end{itemize}

See \cite{Hila08b}.


%========================
% The optimal realization problem
%========================
\section{The optimal realization problem}


The problem of determining the best realization can be posed as follows:
\begin{problem}[Optimal realization problem]
	Consider a trans\-fer function $H$ and a sensitivity measure $\mathcal{J}$.
	The \I{optimal design problem} is to find the best realization $\mathcal{R}_{opt}$ with transfer function $H$ according to the criteria $\mathcal{J}$, that is
	\begin{equation}
		\mathcal{R}_{opt} = \underset{\mathcal{R}\in\mathscr{R}_H}{\arg\ \min\ } \mathcal{J}(\mathcal{R}).
	\end{equation}
\end{problem}

Due to the size of $\mathscr{R}_H$, this problem cannot be solved practically. Indeed, a solution may even have infinite dimension. Hence the following problem is introduced to restrict the search to a particular structuration.

\begin{problem}[Optimal structured realization problem]\label{pb:structured_design}
	The problem to find the optimal structured realization $\mathcal{R}_{opt}^{\mathscr{S}}$, that is
	\begin{equation}
		\mathcal{R}_{opt}^{\mathscr{S}} = \underset{\mathcal{R}\in\mathscr{R}_H^{\mathscr{S}}}{\arg\ \min\ } \mathcal{J}(\mathcal{R}).
	\end{equation}
\end{problem}%



The \I{Inclusion Principle} (Proposition \ref{prop:inclusion_principle2}) provides the means to search over the structured realizations set $\mathscr{R}_H^{\mathscr{S}}$.\\
Since the measure $\mathcal{J}$ could be non-smooth and/or non-convex, the Adaptive Simulated Annealing (ASA) \cite{Ingb96} method could be used  to solve  Problem \ref{pb:structured_design}. This method has worked well for other optimal realization problems \cite{Wu01}. \\
The FWR Toolbox can also use simplex or Quasi-Newton algorithm to solve problem \ref{pb:structured_design}.


%=======
% Tutorial
%=======
\section{Tutorial}


\subsection{First example}
Let's consider this discrete state-space system:
\begin{verbatim}
>> A = [ 1.4590 -0.91037 0.39565; 1 0 0; 0 0.5 0 ];
>> B = [ 0.5; 0; 0 ];
>> C = [ 0.28261 0.13244 0.15183 ];
>> D = 0.0031689;
>> Sys = ss(A,B,C,D, 1e-2);
\end{verbatim}
Then, we can create a first Finite Wordlength Realization
\begin{verbatim}
R = SS2FWR(Sys);
\end{verbatim}
First, let's display this FWR object
\begin{verbatim}
>> R
R has 1 input, 1 output, 3 states, and 0 intermediate variable.
Z=
   1.4590e+00  -9.1037e-01   3.9565e-01   5.0000e-01
   1.0000e+00            0            0            0
            0   5.0000e-01            0            0
   2.8261e-01   1.3244e-01   1.5183e-01   3.1689e-03
\end{verbatim}
It is also possible to display, for example, the associated matrices $Wo$ (observability grammian), $P$ and $W_Z$:
\begin{verbatim}
>> R.Wo
ans =
   1.0232e+00  -6.1354e-01   3.9081e-01
  -6.1354e-01   5.5557e-01  -2.7112e-01
   3.9081e-01  -2.7112e-01   1.8322e-01

>> R.P
ans =
   1.4590e+00  -9.1037e-01   3.9565e-01
   1.0000e+00            0            0
            0   5.0000e-01            0

>> R.WZ
ans =
     1     1     1     1
     0     0     0     0
     0     1     0     0
     1     1     1     1
\end{verbatim}
This last result shows that some coefficients will be considered as exactly implemented (coefficients where $(W_Z)_{ij}$ is null), whereas some will be modified during the quantization.\\
It is possible to ask for the input-output sensitivity and the sensitivity matrix
\begin{verbatim}
>> [M MZ] = MsensH(R)
M =
   1.4391e+01

MZ =
   1.9730e+00   1.9730e+00   9.8651e-01   1.0115e+00
            0            0            0            0
            0   8.4062e-01            0            0
   1.1358e+00   1.1358e+00   5.6788e-01   1.0000e+00
\end{verbatim}
To have the sensitivity for all the coefficients, it is possible to set $W_Z$ to 1
\begin{verbatim}
>> R.WZ=ones(4);
>> [M MZ] = MsensH(R);
>> MZ
MZ =
   1.9730e+00   1.9730e+00   9.8651e-01   1.0115e+00
   1.3716e+00   1.3716e+00   6.8582e-01   7.4536e-01
   8.4062e-01   8.4062e-01   4.2031e-01   4.2805e-01
   1.1358e+00   1.1358e+00   5.6788e-01   1.0000e+00
\end{verbatim}

Now, it could be interesting to find, among the equivalent state-space realizations, one that minimize this input-output sensitivity. For this purpose, we need to create a Finite Wordlength Structuration:
\begin{verbatim}
>> S = SS2FWS( Sys)
 has 1 input, 1 output, 3 states, and 0 intermediate variable.
Z=
   1.4590e+00  -9.1037e-01   3.9565e-01   5.0000e-01
   1.0000e+00            0            0            0
            0   5.0000e-01            0            0
   2.8261e-01   1.3244e-01   1.5183e-01   3.1689e-03
T=
     1     0     0
     0     1     0
     0     0     1
\end{verbatim}
In addition to a FWR object, a FWStructuration includes a parameter \matlab{T}, that allows to get all the state-space equivalent realizations $(T^{-1}AT,T^{-1}B,CT,D)$.\\
To consider a new realization, deduced from the original one with this transformation, we simply give a new value for $T$:
\begin{verbatim}
>> S.T=rand(3)
 has 1 input, 1 output, 3 states, and 0 intermediate variable.
Z=
   3.7455e-01   1.3029e+00  -4.6071e+00  -2.0013e+00
   2.1170e-01  -9.4236e-01   5.5850e+00   5.0983e-01
  -1.2475e-02  -4.9193e-01   2.0268e+00   1.8077e+00
   3.2995e-01   1.8533e-01   3.9119e-01   3.1689e-03
T=
   3.8156e-01   1.8687e-01   6.4631e-01
   7.6552e-01   4.8976e-01   7.0936e-01
   7.9520e-01   4.4559e-01   7.5469e-01
\end{verbatim}
The original realization is stored in \matlab{S.Rini}, whereas the actual realization is obtained with \matlab{S.R}.\\
It is important to remark that all the equivalent realizations deduced from the original one have the same $W_Z$ matrix. So, in order to consider fully parameterized realizations, it is important to set $W_Z$ to a matrix with all coefficients set to 1, with \matlab{S.Rini.WZ=ones(size(S.Rini.WZ));}.\\
It is possible to compare these two realizations and their I/O-sensitivity


\begin{verbatim}
>> MsensH(S.Rini)
ans =
   1.4391e+01

>> MsensH(S.R)
ans =
   5.8668e+02
\end{verbatim}

Now, the most interesting thing is to search for the optimal realization.\\
First, we need to set the options for the search
\begin{verbatim}
>> options = {'method','simplex','Display','Iter','MaxFunEvals',1e4};
\end{verbatim}
Then, we can run the optimization:
\begin{verbatim}
>> S = optim( S, optins, @MsensH)
\end{verbatim}

The options are cells of pairs. Some options concern the \funcName[@FWS/optim]{optim} methods, while some are directly passed to the Matlab optimization algorithm used (\matlab{fminsearch}, \matlab{fminunc}, ...).
\matlab{\{'method','simplex'\}} allows to use the \matlab{fminsearch} algorithm, whereas \matlab{\{'Display','Iter'\}} and \matlab{\{'MaxFunEvals',1e4\}} allow to display each iteration and set the maximum number of function evaluation to $10^4$. See \matlab{optimset} for all the possible options and the method \funcName[@FWS/optim]{optim} for all the possible options.

\matlab{S} has now the \I{optimal} value for $T$.






%===========
% The classes
%===========
\section{The classes}
The the \matlab{FWR toolbox} is based on two classes:
\begin{itemize}
	\item the \matlab{FWR} class to describe the Finite Wordlength Realizations
	\item the \matlab{FWS} class for the Finite Wordlength Structurations
\end{itemize}


%==========================
\subsection{The \matlab{FWR} class}

The \matlab{FWR} class describes a realization expressed with the SIF (see def. \ref{def:realization}). It contains the following fields:
\begin{itemize}
	\item \matlab{l}, \matlab{m}, \matlab{n} and \matlab{p} : dimensions of the realization ($l$ intermediate variables, $m$ inputs, $n$ states and $p$ outputs)
	\item \matlab{J}, \matlab{K}, \matlab{L}, \matlab{M}, \matlab{N}, \matlab{P}, \matlab{Q}, \matlab{R} and \matlab{S} : correspond to matrices $J$ to $S$
	\item \matlab{Z} : the $Z$ matrix defined in \eqref{eq:def_Z} from $J$ to $S$
	\item  \matlab{WJ}, \matlab{WK}, \matlab{WL}, \matlab{WM}, \matlab{WN}, \matlab{WP}, \matlab{WQ}, \matlab{WR} and \matlab{WS} : the weighting matrices $W_J$ to $W_S$  (they imply which parameter is exactly implemented ($0$, $\pm1$, a power of two or any number that will not be changed during the quantization), see \eqref{eq:defW})
	\item \matlab{WZ} : the $W_Z$ matrix
	\item \matlab{AZ}, \matlab{BZ}, \matlab{CZ} and \matlab{AZ} : matrices $A_Z$, $B_Z$, $C_Z$ and $D_Z$ (see equations \eqref{eq:defAZandBZ} and \eqref{eq:defCZandDZ})
	\item \matlab{Wc}, \matlab{Wo} : commandability and observability gramians $W_c$ and $W_o$
	\item \matlab{FPIS} : a \I{Fixed-Point Implementation Scheme} (see section \ref{sec:FPIS})
	\item \matlab{fp}, \matlab{block}, \matlab{rZ} : these fields are related to the coefficient representation
	\begin{itemize}
		\item \matlab{fp} : sets if the implementation uses the fixed-point or the floating-point representation. \matlab{fp} can take the values \matlab{'fixed'} (1=default) or \matlab{'floating'} (2).
		\item \matlab{block} : sets the coefficient's block. Coefficients in the same block share the same representation (binary-point position). \matlab{block} can take the values \matlab{'full'} (1), \matlab{'natural'} (2=default), or \matlab{'none'} (3).
		\item \matlab{rZ} : gives how much $Z$ is changed during the quantization process : $Z$ is perturbed to $Z+r_Z\times\Delta$ where
 	\begin{equation}
 		r_Z \triangleq
 		\begin{cases}
 			W_Z & \text{for fixed-point representation,} \\
 			2\eta_Z \times W_Z & \text{for floating-point representation,}
 		\end{cases}
 	\end{equation}
 	and $\eta_Z$ is such that
 	\begin{equation}
 		\pa{\eta_Z}_{i,j} \triangleq
 			\left\lbrace\begin{array}{l}
 				\text{the largest absolute value of}\\
 				\text{the block in which }  Z_{i,j} \text{ resides.}
 			\end{array}\right.
 	\end{equation}
	\end{itemize}
		These fields are only used for the sensitivities measure (\funcName[@FWR/MsensH]{MsensH}, \funcName[@FWR/MsensPole]{MsensPole}, ...), and are independent from the FPIS. These fields will probably disapear.\\
		See \cite{Hila07b} for \I{block-fixed-point} and \I{block-floating-point} representation.

\end{itemize}

When a FWR object is created, it is not possible to change its dimensions (fields \matlab{l}, \matlab{m}, \matlab{n} and \matlab{p} or the size of the matrices) or the fields \matlab{AZ}, \matlab{BZ}, \matlab{CZ}, \matlab{AZ}, \matlab{Wc} and \matlab{Wo}.\\
Fields \matlab{Z} and \matlab{WZ} are redundant with fields \matlab{J} to \matlab{S} and \matlab{WJ} to \matlab{WS}, but they can both be usefull. Changing \matlab{Z} automatically changes fields \matlab{J} to \matlab{S} and reciprocally (the same with \matlab{WZ}). Fields \matlab{AZ}, \matlab{BZ}, \matlab{CZ}, \matlab{AZ}, \matlab{Wc} and \matlab{Wo} are deduced accordingly.

%
\subsubsection{Fixed-Point Implementation Scheme}\label{sec:FPIS}

The FPIS is a structure to set the \I{Fixed-Point Implementation Scheme}. It is composed by:
 	\begin{itemize}
 		\item the fixed-point format of the input $(\beta_U,\gamma_U)$
 		and its maximum magnitude value $\overset{\max}{U}$
 		\item the fixed-point format of the intermediate variables $(\beta_T,\gamma_T)$
 		\item the fixed-point format of the states $(\beta_X,\gamma_X)$
 		\item the fixed-point format of the output $(\beta_Y,\gamma_Y)$
 		\item the fixed-point format of the coefficients $(\beta_Z,\gamma_Z)$
 		\item the fixed-point format of the accumulator $(\beta_{ADD}+\beta_{G},\gamma_{ADD})$ ($\beta_G$ guard bits)
 		\item the right-shift bits after each scalar product $d_{ADD}$ (\matlab{shiftADD})
 		\item the right-shift bits after each multiplication by a coefficient $d_Z$ (\matlab{shiftZ})
 		\item the computational scheme : \I{Roundoff After Multiplication} (RAM) or \I{Roundoff Before Multiplication} (RBM)
 	\end{itemize}

 	The algorithm
 	\begin{align*}
 		&\text{[i]} & JT(k+1) & \leftarrow MX(k) + NU(k)\\
 		&\text{[ii]} & X(k+1)  & \leftarrow KT(k+1) + PX(k) + QU(k)\\
 		&\text{[iii]} & Y(k)    & \leftarrow LT(k+1) + RX(k) + SU(k)
 	\end{align*}
 	requires to implement $l+n+p$ scalar products.\\
 	Each scalar product
 	\begin{equation}
 		S = \sum_{i=1}^n P_i E_i
 	\end{equation}
 	where $\pa{P_i}_{1 \leq i \leq n}$ are given coefficients and
 	$\pa{E_i}_{1 \leq i \leq n}$ some bounded variables, can be
 	implemented according to the following algorithms \ref{algo:setFPIS:RAM} and
 	\ref{algo:setFPIS:RBM}, and where $P'_i$, $E'_i$ and $S'_i$ are the integer representations
 	(according to their fixed-point format) to $P_i$,$E_i$ and
 	$S_i$.\\
	\begin{multicols}{2}{
 	\begin{algorithm}[H]
		\caption{\I{Roundoff After Multiplication} (RAM)\label{algo:setFPIS:RAM}}
 		$Add\leftarrow 0$\\
 		\For{$i$ from 0 to $n$}{$Add\leftarrow \pa{ P_i' * E_i' } >> d_i$}
 		$S'_i \leftarrow Add >> d'_i$
 	\end{algorithm} \ \\
 	\begin{algorithm}[H]
		\caption{\I{Roundoff Before Multiplication} (RBM)\label{algo:setFPIS:RBM}}
 		$Add\leftarrow 0$\\
 		\For{$i$ from 0 to $n$}{$Add\leftarrow \pa{ P_i' >> d_i } * E_i'$}
 		$S'_i \leftarrow Add >> d_i'$
 	\end{algorithm}}
	\end{multicols}
	Of course, $d_i$ represents the right-shift after each multiplication and $d'_i$ represents the final shift. They respectively correspond to the $d_Z$ and $d_{ADD}$ shift in the SIF algorithm.

 	The user may specify all the wordlengths ($\beta_U$, $\beta_T$, $\beta_X$, $\beta_Y$, $\beta_{ADD}$, $\beta_g$
 	and $\beta_Z$) and $\overset{\max}{U}$.
	
	See \cite{Hila08c} and the function \funcName[@FWR/setFPIS]{setFPIS} for more details.



%
\subsubsection{Methods}

The FWR's methods are : \\
\input{SimpleDoc/@FWR/table_SD}


%=========================
\subsection{The \matlab{FWS} class}\label{sec:FWSclass}

The \matlab{FWS} class describes a structuration and the way to search over the set of equivalent realizations of this structuration. This is done by defining the parameters used to search over the equivalent set and how theses parameters give new realizations (via the $\mt{U}\mt{Y}\mt{W}$-transformation, eq. \eqref{eq:UYWtransformation}, or directly).

%When creating a structuration, the goal is to help to manipulate equivalent realizations in that structuration (set of realizations with a given structure). For example, the equivalent state-space realizations can be handled with a structuration created by the command
%\begin{verbatim}
%	S = SS2FWS( ss(A,B,C,D,Te) );
%\end{verbatim}
%Then, it is possible to consider a realization, with state-space structure, from  a transformation matrix $T$


It contains the following fields :
\begin{itemize}
	\item \matlab{Rini} : the initial realization of this structuration (all the other equivalent structured realizations are computed \I{from} this realization),
	\item \matlab{R} : the actual considered realization,
	\item \matlab{paramsName} : name of the parameters used to search over the equivalent set,
	\item \matlab{paramsValue} : value of this parameters (they define the actual considered realization \matlab{R}),
	\item \matlab{paramsSize} : size of these parameters,
	\item \matlab{indices} : vector of indices. To do the optimization, the values of the parameters are put together in a row vector, and \matlab{indices} stores these indices, in that row, of each parameters (it is used when some parameters are fixed during the optimization). This is also internally used when some parameters are fixed,
	\item \matlab{UYWfun} : handle to a function that gives the transformation matrices $\mt{U}$, $\mt{Y}$ and $\mt{W}$ from the parameters,
	\item \matlab{Rfun} : handle to a function that gives the realization from the parameters (when the \matlab{UYWfun} cannot be defined),
	\item \matlab{dataMeasure} : cell of extra data used for the FWL measures (to store values that do not change from one realization to another equivalent) : every FWL measure can add what it needs inside,
	\item \matlab{dataFWS} : cell of extra data used to store internal values (can be used when defining a structuration).
\end{itemize}

The parameters (their names, values and sizes are stored in \matlab{paramsName}, \matlab{paramsValue} and \matlab{paramsSize}) can be used directly by their names, like any other fields : if \matlab{S} is a structuration, with parameters named \matlab{'T1'} and \matlab{'T2'}, expressions \matlab{S.T1} and \matlab{S.T2} allow to access the corresponding values.\\
All the fields of the \matlab{FWS} class are accessible with the \matlab{.fieldname} method (\matlab{get}), but it is only possible to change \matlab{Rini}  and the parameters (it is not possible to change their sizes). And changing one parameter will automatically change \matlab{R}.

When a structuration \matlab{S} is defined, it is then possible to search over all the equivalent realizations with the same structuration by changing the values of the associated parameters (those where the names are in \matlab{paramsName}). \matlab{S.R} gives the new realization (automatically update when \matlab{paramsValue} change, with \matlab{S.\I{paramName}=...}). This can be done with the \funcName[@FWS/optim]{optim} method, that uses quasi-Newton, simplex or ASA algorithm.

There is two ways to define how the parameters give a new realization. Only one of the two fields \matlab{UYWfun} and  \matlab{Rfun} must be filled. The function \matlab{UYWfun} must be preferred, because it allows to compute more quickly the FWL measures whose comportment with the $\mt{UYW}$-transformation is defined.

%
\subsubsection{\matlab{UYWfun} function}
	\desc{Purpose}
		The \matlab{UYWfun} is a function that defines how to search over all the equivalent realizations. It transforms the parameters of the structuration (given by \matlab{args}) in transformation matrices $\mt{U}$, $\mt{Y}$ and $\mt{W}$. The \matlab{cost\_flag} indicates if the transformation is valid.
	\desc{Syntax}
		\matlab{function [U,Y,W,cost\_flag] = \I{my\_UYW\_fun}( Rini, paramsValue, dataFWS)}
	\desc{Arguments}
		\begin{tabular}{l@{\ :\ }l}
			\matlab{U,Y,W} & matrices $\mt{U}$, $\mt{Y}$ and $\mt{W}$ \\
			\matlab{cost\_flag} & boolean that indicates if the parameters forms a valid transformation \\
			\matlab{Rini} & initial FW Realization \\
			\matlab{paramsValue} & cells of parameters values\\
			\matlab{dataFWS} & cell of extra data (that can be used to store internal values)
		\end{tabular}\\		

The parameters' values are accessible with \matlab{paramsValue} : the $i$\textsuperscript{th} parameter (in the order it is built) is given by \matlab{paramsValue\{i\}}. Most of the time, \matlab{Rini} and \matlab{dataFWS} are not useful (\matlab{U}, \matlab{Y} and \matlab{W} often directly depend on \matlab{paramsValue}).

%
\subsubsection{\matlab{Rfun} function}
	\desc{Purpose}
		The \matlab{Rfun} is a function that directly creates a new realization from the parameters' values, without returning $\mt{U}$, $\mt{Y}$ and $\mt{W}$ matrices.
	\desc{Syntax}
		\matlab{function [R,cost\_flag] = \I{my\_R\_fun}( Rini, paramsValue, dataFWS)}
	\desc{Arguments}
		\begin{tabular}{l@{\ :\ }l}
			\matlab{R} & new realization \\
			\matlab{cost\_flag} & boolean that indicates if the parameters forms a valid transformation \\
			\matlab{Rini} & initial FW Realization \\
			\matlab{paramsValue} & cells of parameters' values \\
			\matlab{dataFWS} & cell of extra data (that can be used to store internal values)
		\end{tabular}\\	

Everyone who creates a FWS should follow theses definitions.

%
\subsubsection{Methods}
The FWS's methods are : \\
\input{SimpleDoc/@FWS/table_SD}





%==========================
% FWR functions/methods reference
%==========================
\newpage
\section{FWR Toolbox reference}

% create
\subsection{create realizations and structurations}
In additions to the FWR/FWS's methods, the following functions can be used to create classical realizations and structurations : \\

\input{SimpleDoc/table_SD}

Here is the detailed list:

\input{SimpleDoc/main_SD}


\subsection{Private functions}
Some functions internally used are described here:
\input{SimpleDoc/private/table_SD}
\input{SimpleDoc/private/main_SD}


% FWR
\newpage
\subsection{\matlab{FWR} class methods }
\input{SimpleDoc/@FWR/table_SD}
\input{SimpleDoc/@FWR/main_SD}

\subsection{\matlab{FWR} private functions}
\input{SimpleDoc/@FWR/private/table_SD}
\input{SimpleDoc/@FWR/private/main_SD}

% FWS
\newpage
\subsection{\matlab{FWS} class methods }
\input{SimpleDoc/@FWS/table_SD}
\input{SimpleDoc/@FWS/main_SD}

\subsection{\matlab{FWS} private functions}
\input{SimpleDoc/@FWS/private/table_SD}
\input{SimpleDoc/@FWS/private/main_SD}




%==========
% Bibliography
%==========
\section{Bibliography}

\bibliographystyle{plain}
\bibliography{FWL,codes,finite_precision,jfw}
		
\end{document} 